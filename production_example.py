#!/usr/bin/env python3
"""
Production RAG Pipeline Example

This script demonstrates how to use the complete RAG pipeline with real LLM integration
for production deployment. It shows the exact flow described in your AI prompt.

Usage:
    python production_example.py

Environment Variables:
    OPENAI_API_KEY - Your OpenAI API key
    AZURE_OPENAI_API_KEY - Your Azure OpenAI API key  
    AZURE_OPENAI_ENDPOINT - Your Azure OpenAI endpoint
    ANTHROPIC_API_KEY - Your Anthropic API key
"""

import os
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from rag_papers.generation.prompts import QueryProcessor
from rag_papers.generation.generator import RAGGenerator, MockGenerator
from rag_papers.generation.verifier import ResponseVerifier

# Import production LLM generators (if available)
try:
    from rag_papers.generation.production_llm import (
        LLMFactory, create_production_rag_generator, load_llm_config_from_env
    )
    PRODUCTION_AVAILABLE = True
except ImportError:
    PRODUCTION_AVAILABLE = False


def demonstrate_exact_ai_prompt_flow():
    """
    Demonstrate the exact flow from your AI prompt:
    User Query ‚Üí Prompt Creation ‚Üí Response Generation ‚Üí Quality Check ‚Üí Final Answer
    """
    
    print("=" * 70)
    print("ü§ñ Production RAG Pipeline - AI Prompt Implementation")
    print("=" * 70)
    
    # Step 1: Initialize components
    print("\\nüîß Step 1: Initializing Pipeline Components")
    print("   üìù prompts.py - Dynamic prompt creation")
    print("   ‚ö° generator.py - LLM integration")  
    print("   üîç verifier.py - Quality verification")
    
    query_processor = QueryProcessor()
    response_verifier = ResponseVerifier()
    
    # Try to use production LLM, fallback to mock
    if PRODUCTION_AVAILABLE and has_api_keys():
        print("   üöÄ Using PRODUCTION LLM integration")
        rag_generator = create_production_rag_generator()
    else:
        print("   üß™ Using MOCK generator (add API keys for production LLMs)")
        rag_generator = RAGGenerator(generator=MockGenerator())
    
    # Sample context (simulates retrieved documents from your RAG system)
    context = """
    Deep learning is a subset of machine learning that uses artificial neural networks 
    with multiple layers (hence "deep") to model and understand complex patterns in data. 
    It's inspired by the structure and function of the human brain, using interconnected 
    nodes (neurons) organized in layers.
    
    Key characteristics of deep learning:
    1. Hierarchical learning: Each layer learns increasingly complex features
    2. Automatic feature extraction: No need for manual feature engineering
    3. Large data requirements: Typically needs massive datasets to train effectively
    4. Computational intensity: Requires significant processing power (GPUs)
    
    Common applications include:
    - Computer vision (image recognition, object detection)
    - Natural language processing (translation, chatbots)
    - Speech recognition and synthesis
    - Autonomous vehicles
    - Medical diagnosis and drug discovery
    """
    
    # Test query from your AI prompt example
    user_query = "What is deep learning?"
    
    print(f"\\nüìù User Query: '{user_query}'")
    print(f"üìÑ Context: {len(context)} characters of retrieved content")
    
    # Step 2: Prompt Creation (prompts.py)
    print("\\nüß† Step 2: Prompt Creation (prompts.py)")
    processed_query = query_processor.preprocess_query(user_query)
    
    print(f"   ‚úÖ Intent detected: {processed_query.intent.value}")
    print(f"   ‚úÖ Keywords extracted: {', '.join(processed_query.keywords[:5])}")
    print(f"   ‚úÖ Entities found: {', '.join(processed_query.entities[:3])}")
    print(f"   ‚úÖ Query normalized: '{processed_query.normalized_query}'")
    
    # Step 3: Response Generation (generator.py)
    print("\\n‚ö° Step 3: Response Generation (generator.py)")
    print("   üì§ Sending prompt to LLM...")
    
    generated_response = rag_generator.generate_answer(
        context=context,
        query=user_query,
        processed_query=processed_query
    )
    
    print(f"   ‚úÖ Response generated by: {generated_response.model_used}")
    print(f"   ‚úÖ Response length: {len(generated_response.text)} characters")
    print(f"   ‚úÖ Model confidence: {generated_response.confidence:.2f}")
    
    # Step 4: Quality Check (verifier.py)
    print("\\nüîç Step 4: Quality Check (verifier.py)")
    quality_score = response_verifier.verify_response(
        response=generated_response.text,
        query=user_query,
        context=context
    )
    
    print(f"   üìä Overall quality score: {quality_score.overall_score:.3f}")
    print(f"   üìä Relevance score: {quality_score.relevance_score:.3f}")
    print(f"   üìä Coherence score: {quality_score.coherence_score:.3f}")
    print(f"   üìä Completeness score: {quality_score.completeness_score:.3f}")
    print(f"   üîç Quality issues detected: {len(quality_score.issues)}")
    
    # Quality threshold check
    is_acceptable = response_verifier.is_acceptable(quality_score, threshold=0.7)
    
    if is_acceptable:
        print("   ‚úÖ PASSED: Response meets quality criteria!")
    else:
        print("   ‚ö†Ô∏è  FLAGGED: Response needs improvement")
        if quality_score.suggestions:
            print("   üí° Suggestions:")
            for suggestion in quality_score.suggestions[:2]:
                print(f"      ‚Ä¢ {suggestion}")
    
    # Step 5: Final Answer
    print("\\nüéØ Step 5: Final Answer")
    print("=" * 50)
    print(generated_response.text)
    print("=" * 50)
    
    # Summary
    print("\\nüìã Pipeline Summary:")
    print(f"   ‚Ä¢ Query processed in: prompts.py (intent: {processed_query.intent.value})")
    print(f"   ‚Ä¢ Response generated by: generator.py ({generated_response.model_used})")
    print(f"   ‚Ä¢ Quality verified by: verifier.py (score: {quality_score.overall_score:.3f})")
    print(f"   ‚Ä¢ Final status: {'‚úÖ Approved' if is_acceptable else '‚ö†Ô∏è  Needs review'}")


def demonstrate_multiple_query_types():
    """Demonstrate the pipeline with different query types as mentioned in the AI prompt."""
    
    print("\\n" + "=" * 70)
    print("üéØ Multiple Query Types Demonstration")
    print("=" * 70)
    
    # Initialize components
    query_processor = QueryProcessor()
    
    if PRODUCTION_AVAILABLE and has_api_keys():
        rag_generator = create_production_rag_generator()
    else:
        rag_generator = RAGGenerator(generator=MockGenerator())
    
    response_verifier = ResponseVerifier()
    
    # Test different query types
    test_cases = [
        {
            "query": "What is machine learning?",
            "context": "Machine learning is a method of data analysis that automates analytical model building...",
            "expected_intent": "definition"
        },
        {
            "query": "Explain how neural networks learn",
            "context": "Neural networks learn through a process called backpropagation, adjusting weights based on errors...",
            "expected_intent": "explanation"
        },
        {
            "query": "Compare supervised vs unsupervised learning",
            "context": "Supervised learning uses labeled data while unsupervised learning finds patterns in unlabeled data...",
            "expected_intent": "comparison"
        },
        {
            "query": "How to implement a CNN?",
            "context": "To implement a CNN, start by defining convolutional layers, then pooling layers, followed by dense layers...",
            "expected_intent": "how_to"
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\\n{'-' * 40}")
        print(f"Test Case {i}: {test_case['query']}")
        print(f"{'-' * 40}")
        
        # Process query
        processed = query_processor.preprocess_query(test_case['query'])
        print(f"Detected Intent: {processed.intent.value}")
        
        # Generate response
        response = rag_generator.generate_answer(
            context=test_case['context'],
            query=test_case['query'],
            processed_query=processed
        )
        
        # Verify quality
        quality = response_verifier.verify_response(
            response.text, test_case['query'], test_case['context']
        )
        
        print(f"Quality Score: {quality.overall_score:.3f}")
        print(f"Response: {response.text[:100]}...")
        
        # Check if intent matches expectation
        intent_match = processed.intent.value == test_case['expected_intent']
        print(f"Intent Detection: {'‚úÖ' if intent_match else '‚ö†Ô∏è'} {processed.intent.value}")


def has_api_keys() -> bool:
    """Check if any production API keys are available."""
    return any([
        os.getenv("OPENAI_API_KEY"),
        os.getenv("AZURE_OPENAI_API_KEY"),
        os.getenv("ANTHROPIC_API_KEY")
    ])


def show_configuration_guide():
    """Show how to configure for production use."""
    
    print("\\n" + "=" * 70)
    print("‚öôÔ∏è  Production Configuration Guide")
    print("=" * 70)
    
    print("\\nüîë API Key Configuration:")
    print("Set environment variables for your preferred LLM provider:")
    print()
    print("For OpenAI:")
    print("   export OPENAI_API_KEY='your-openai-api-key'")
    print()
    print("For Azure OpenAI:")
    print("   export AZURE_OPENAI_API_KEY='your-azure-key'")
    print("   export AZURE_OPENAI_ENDPOINT='https://your-resource.openai.azure.com'")
    print("   export AZURE_OPENAI_DEPLOYMENT='your-deployment-name'")
    print()
    print("For Anthropic:")
    print("   export ANTHROPIC_API_KEY='your-anthropic-key'")
    
    print("\\nüîß Usage Examples:")
    print('''
# Using OpenAI
from rag_papers.generation.production_llm import LLMFactory
generator = LLMFactory.create_openai_generator(model_name="gpt-4")

# Using Azure OpenAI  
generator = LLMFactory.create_azure_generator(
    deployment_name="gpt-4",
    api_version="2023-12-01-preview"
)

# Using Anthropic
generator = LLMFactory.create_anthropic_generator(model_name="claude-3-sonnet-20240229")
''')


def main():
    """Main demonstration function."""
    
    try:
        # Core demonstration from AI prompt
        demonstrate_exact_ai_prompt_flow()
        
        # Multiple query types
        demonstrate_multiple_query_types()
        
        # Configuration guide
        show_configuration_guide()
        
        print("\\n" + "=" * 70)
        print("üéâ Production RAG Pipeline Demonstration Complete!")
        print("=" * 70)
        
        print("\\n‚ú® Implementation matches your AI prompt requirements:")
        print("   ‚úÖ prompts.py - Dynamic prompt creation based on query type")
        print("   ‚úÖ generator.py - LLM integration with multiple providers")
        print("   ‚úÖ verifier.py - Heuristic scoring (0-1) for quality assessment")
        print("   ‚úÖ Modular and maintainable architecture")
        print("   ‚úÖ Robust error handling and fallback mechanisms")
        print("   ‚úÖ Support for multiple query types and contexts")
        
        if not has_api_keys():
            print("\\nüí° To use production LLMs, set API keys in environment variables")
            print("   Currently using mock generator for demonstration")
        else:
            print("\\nüöÄ Production LLM integration active!")
            
    except Exception as e:
        print(f"\\n‚ùå Error during demonstration: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())